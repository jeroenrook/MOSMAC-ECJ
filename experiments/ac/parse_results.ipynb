{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pygmo\n",
    "from pymoo.indicators.igd_plus import IGDPlus\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import scenarios as scenariodef\n",
    "from multiprocess import Pool\n",
    "# import configurators\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "resultdir = Path(\"results\")\n",
    "figurepath = Path(\"figures\")\n",
    "figurepath.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Definitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "configurators = [\"RandomSearch\", \"RandomSearchSI\", \"RandomSearch25\", \"default\",  \"SMAC\", \"MO-ParamILS\",  \"ParEGO\", \"MO-SMAC\", \"MO-SMAC-PHVI\"]\n",
    "\n",
    "# color_palette = [(230, 159, 0), (86, 180, 233), (0, 158, 115), (240, 228, 66), (0, 114, 178), (213, 94, 0), (204, 121, 167)]  # Wong color palette (works for color-blinded people)\n",
    "color_palette = [(51, 34, 136), (17, 119, 51), (68, 170, 156), (136, 204, 238), (221, 204, 119), (204, 102, 119), (170, 68, 153), (136, 34, 85),][::-1]  # Tol colorblind palette (https://davidmathlogic.com/colorblind/#%23332288-%23117733-%2344AA99-%2388CCEE-%23DDCC77-%23CC6677-%23AA4499-%23882255)\n",
    "color_palette = [tuple([float(f\"{channel / 255:.3f}\") for channel in color]) for color in color_palette]\n",
    "color_palette += color_palette\n",
    "# color_palette = sns.color_palette(palette=\"colorblind\")\n",
    "markers = [\"s\", \"o\", \"X\", \"P\", \"^\", \"v\", \">\", \"<\", \"*\"][::-1]\n",
    "\n",
    "\n",
    "\n",
    "def get_style(conf, key):\n",
    "    return style_guide[conf][key]\n",
    "\n",
    "# display(style_guide)\n",
    "\n",
    "config_names = {\n",
    "    \"MO-SMAC-PHVI\": \"MO-SMAC-PHVI\",\n",
    "    \"ParEGO\":  \"MO-SMAC-PE\",\n",
    "    \"MO-SMAC\": \"MO-SMAC-EHVI\",\n",
    "    \"MO-ParamILS\": \"MO-ParamILS\",\n",
    "    \"SMAC\": \"SMAC\",\n",
    "    \"RandomSearchSI\": \"Random Search + MO-Intensify\",\n",
    "    \"RandomSearch\": \"Random Search\",\n",
    "    \"RandomSearch25\": \"Random Search\",\n",
    "    \"default\": \"Default\",\n",
    "}\n",
    "\n",
    "style_guide = {}\n",
    "for i, conf in enumerate(config_names.values()):\n",
    "    style_guide[conf] = {\"color\": color_palette[i%len(color_palette)], \"marker\": markers[i%len(markers)]}\n",
    "\n",
    "print(style_guide)\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "# matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# #matplotlib.use(\"pgf\")\n",
    "# matplotlib.rcParams.update({\n",
    "#     #\"pgf.texsystem\": \"pdflatex\",\n",
    "#     'font.family': 'serif',\n",
    "#     # 'text.usetex': True,\n",
    "#     # 'pgf.rcfonts': False,\n",
    "# })\n",
    "\n",
    "scenario_replacements = {\n",
    "    \"EA_OO_MMMOOP\": \"EMOA-(hv, sp)\",\n",
    "    \"MIP_CPLEX_REGIONS200_cutoff\": \"MIP-(gap, cutoff)\",\n",
    "    \"MIP_CPLEX_REGIONS200_runtime\": \"MIP-(gap, runtime)\",\n",
    "    \"MIP_CPLEX_REGIONS200_CONT_cutoff\": \"MIP-(gap, cutoff) (c)\",\n",
    "    \"MIP_CPLEX_REGIONS200_CONT_runtime\": \"MIP-(gap, runtime) (c)\",\n",
    "    \"SAT_CMS_QUEENS_runtime_memory\": \"SAT-(runtime, memory)\",\n",
    "    \"SAT_CMS_QUEENS_runtime_solved\": \"SAT-(runtime, solved)\",\n",
    "    \"ML_RF_STUDENTS_precision_recall\": \"ML-(precision, recall)\",\n",
    "    \"ML_RF_STUDENTS_precision_recall_size\": \"ML-(precision, recall, size)\",\n",
    "    \"ML_RF_STUDENTS_accuracy_size\": \"ML-(accuracy, size)\",\n",
    "}\n",
    "\n",
    "configurator_replacements = config_names\n",
    "# {\n",
    "#     \"RandomSearch\": \"Random Search\",\n",
    "#     \"RandomSearchSI\": \"Random Search + MO-Intensify\",\n",
    "#     \"default\": \"Default\",  \n",
    "#     # \"SMAC\": ,\n",
    "#     # \"MO-ParamILS\",:  \n",
    "#     \"ParEGO\": \"MO-SMAC-PE\",\n",
    "#     \"MO-SMAC-ParEGO\": \"MO-SMAC-PE\",\n",
    "#     \"MO-SMAC\": \"MO-SMAC-EHVI\",\n",
    "#     # \"MO-SMAC-PHVI\": ,\n",
    "# }\n",
    "\n",
    "palette = {conf: col for conf, col in zip(config_names.values(), color_palette)}\n",
    "markers = {conf: col for conf, col in zip(config_names.values(), markers)}\n",
    "order = configurators  #  [conf for conf in list(config_names.keys()) if conf in list(qdf[\"configurator\"].unique())]\n",
    "\n",
    "\n",
    "def rename_metadata(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for k, v in scenario_replacements.items():\n",
    "        df.loc[df[\"scenario\"] == k, \"scenario\"] = v\n",
    "    for k, v in configurator_replacements.items():\n",
    "        df.loc[df[\"configurator\"] == k, \"configurator\"] = v\n",
    "    return df\n",
    "\n",
    "print(f\"{config_names=}\")\n",
    "print(f\"{palette=}\")\n",
    "print(f\"{markers=}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Legend"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ncols = len(configurators)\n",
    "\n",
    "fig = plt.figure(figsize=(ncols, 1), dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "handles = []\n",
    "handles_small = []\n",
    "legend_names = []\n",
    "\n",
    "for basename, conf in config_names.items():\n",
    "    if basename in [\"MO-SMAC\", \"RandomSearchSI\", \"RandomSearch\"]:\n",
    "        continue\n",
    "    style = style_guide[conf]\n",
    "    print(conf, style)\n",
    "    handles.append(Line2D([0], [0], linestyle=\"\", markersize=14, **style))\n",
    "    handles_small.append(Line2D([0], [0], linestyle=\"\", markersize=10, **style))\n",
    "    legend_names.append(configurator_replacements.get(conf, conf))\n",
    "\n",
    "fig.legend(handles, legend_names, loc=\"upper center\", fontsize=18, frameon=False, columnspacing=1.15, ncol=ncols)  # ncols=ncols\n",
    "fig.tight_layout()\n",
    "fig.savefig(figurepath / \"legend.pdf\", bbox_inches=\"tight\", dpi=300, pad_inches=0)\n",
    "plt.show()\n",
    "\n",
    "textwidth = 404 * 2.4# pt\n",
    "pt_per_in = 0.0138888889 \n",
    "textwidth_in = textwidth * pt_per_in\n",
    "n_figs_per_row = 4.25\n",
    "figwidth = textwidth_in / n_figs_per_row\n",
    "figsize = (figwidth, figwidth * 0.8)\n",
    "fig = plt.figure(figsize=figsize, dpi=300)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "fig.legend(handles_small, legend_names, loc=\"center\", fontsize=11, frameon=False, columnspacing=1.15, ncol=1)  # ncols=ncols\n",
    "# fig.tight_layout()\n",
    "fig.savefig(figurepath / \"legend2.pdf\", bbox_inches=\"tight\", dpi=300, pad_inches=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance results\n",
    "Configuration procedure:\n",
    "- Run a configurator $n$ times on a scenario with a different random seed. (We used $n=10$)\n",
    "- Validate the final incumbent (single or multiple configurations) of each configuration run on a common validation set (subset of the train set).\n",
    "- Combine all the incumbents into one archive and throw out all the dominated configurations. This configuration in the archive are non-dominated.\n",
    "- Validate configurations in the filtered archive on the training set.\n",
    "\n",
    "HV computation:\n",
    "- Determine reference point by choosing the largest obtained mean value for an objective by all validation and training runs of all configurations found by all configurators.\n",
    "- Compute the HV.\n",
    "\n",
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scenario_files = [d for d in resultdir.joinpath(\"collect\").iterdir()]\n",
    "scenarios = []\n",
    "for i, s in enumerate(scenario_files):\n",
    "    print(f\"###### {s}\")\n",
    "    dfpath = f\"{resultdir}/collect/dataframes/{Path(s).name}.csv\"\n",
    "    metapath = f\"{resultdir}/collect/metadata/{Path(s).name}.json\"\n",
    "\n",
    "    loaded_from_csv: bool = False\n",
    "    try:\n",
    "        with open(s, \"rb\") as fh:\n",
    "            scenario = pickle.load(fh)\n",
    "    except:\n",
    "        # try json and csv\n",
    "        print(\"Try csv\")\n",
    "        if not (Path(dfpath).exists() and Path(metapath).exists()):\n",
    "            print(f\"Couldn't load {s}\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(dfpath)\n",
    "        loaded_from_csv = True\n",
    "        with open(metapath, \"r\") as f:\n",
    "            scenario = json.load(f)\n",
    "\n",
    "        scenario[\"run_result\"] = df\n",
    "        pass\n",
    "    \n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "\n",
    "    print(f\"> {scenario['experiment']['scenarios']}\")\n",
    "\n",
    "    if scenario[\"experiment\"][\"scenarios\"] in [\"MIP_CPLEX_REGIONS200_CONT_cutoff\",\n",
    "                                               \"MIP_CPLEX_REGIONS200_CONT_runtime\",\n",
    "                                               \"SAT_CMS_QUEENS_runtime_memory\",\n",
    "                                               \"ML_RF_STUDENTS_precision_recall\",\n",
    "                                               \"ML_RF_STUDENTS_precision_recall_size\",\n",
    "                                               \"ML_RF_STUDENTS_accuracy_size\",\n",
    "                                               \"EA_OO_MMMOOP\"\n",
    "                                               ]:\n",
    "        df = scenario[\"run_result\"]\n",
    "        df = df[~df[\"configurators\"].isin([\"MO-SMAC\", \"RandomSearchSI\", \"RandomSearch\"])]\n",
    "\n",
    "        df[\"seeds\"] = df[\"seeds\"].astype(int)\n",
    "        print(\"seeds\", list(df[\"seeds\"].unique()))\n",
    "\n",
    "        df = df[df[\"seeds\"] <  20]\n",
    "        df.loc[df[\"status\"] == \"CRASHED\", objectives] = df[df[\"status\"] != \"CRASHED\"][objectives].max()\n",
    "        scenario[\"run_result\"] = df\n",
    "        scenarios.append(scenario)\n",
    "\n",
    "        #export\n",
    "        for p in [dfpath, metapath]:\n",
    "            Path(p).parent.mkdir(parents=True, exist_ok=True )\n",
    "        df = scenario[\"run_result\"]\n",
    "        if not loaded_from_csv:\n",
    "            df.to_csv(dfpath, index=False)\n",
    "        meta_data = copy.copy(scenario)\n",
    "        del meta_data[\"run_result\"]\n",
    "        with open(metapath, \"w\") as f:\n",
    "            json.dump(meta_data, f)\n",
    "    \n",
    "    else:\n",
    "        print(\">> Skipping..\")\n",
    "\n",
    "print(f\"Got {len(scenarios)} scenarios\")\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for k, v in scenario_replacements.items():\n",
    "        scenario[\"run_result\"].loc[scenario[\"run_result\"][\"scenarios\"] == k, \"scenarios\"] = v\n",
    "    for k, v in configurator_replacements.items():\n",
    "        scenario[\"run_result\"].loc[scenario[\"run_result\"][\"configurators\"] == k, \"configurators\"] = v\n",
    "    \n",
    "\n",
    "scenarios = sorted(scenarios, key=lambda s: s[\"experiment\"][\"scenarios\"])  # Sort by name\n",
    "\n",
    "[s[\"experiment\"][\"scenarios\"] for i, s in enumerate(scenarios)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[s[\"experiment\"][\"scenarios\"] for i, s in enumerate(scenarios)]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Sanity checking\n",
    "for scenario in scenarios:\n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    # if scenario_name in [\"SAT_CMS_QUEENS_runtime_memory\"]:\n",
    "    #     continue\n",
    "    print(f\"{'scenario':15}: {scenario_name}\")\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "    print(f\"{'objectives':15}: {objectives}\")\n",
    "\n",
    "    df = scenario[\"run_result\"]\n",
    "    print(f\"{'configurators':15}: {df['configurators'].unique()}\")\n",
    "\n",
    "    print(df.groupby(\"configurators\")[\"seeds\"].unique())\n",
    "    print(df.groupby([\"configurators\",\"status\"]).size())\n",
    "    # print(df.groupby([\"configurators\",\"action\"])[\"instance\"].nunique())\n",
    "    # print(df.groupby([\"configurators\",\"seeds\",\"configuration\"])[\"instance\"].size())\n",
    "\n",
    "#     print(\"-\"*64)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# keys = [\"configurators\", \"seeds\", \"configuration\"]\n",
    "# perf = df.groupby(keys)[[\"PAR10\", \"memory\"]].mean()\n",
    "#\n",
    "# best_run  = perf.sort_values(\"PAR10\").index[0]\n",
    "#\n",
    "# df[list(map(lambda x: all(x), zip(*[df[k] == v for k, v in zip(keys, best_run)])))]\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Timeouts\n",
    "param = []\n",
    "keys = [\"action\", \"configurators\", \"seeds\", \"configuration\"]\n",
    "tdf = df.sort_values(keys).groupby(keys)[\"status\"].value_counts().unstack(\"status\")\n",
    "tdf[\"Percentage TO\"] = tdf[\"TIMEOUT\"] / (tdf[\"TIMEOUT\"] + tdf[\"SUCCESS\"])\n",
    "\n",
    "tdf.reset_index().groupby([\"action\", \"configurators\"])[\"Percentage TO\"].min()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = scenarios[0][\"run_result\"]\n",
    "\n",
    "df.groupby([\"configurators\",\"seeds\",\"configuration\"])[\"PAR10\"].min().unstack(\"configurators\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of final front and performance computation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scenario = scenarios[-1]\n",
    "scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "print(scenario_name)\n",
    "scen = getattr(scenariodef, scenario_name)()\n",
    "objectives = [o[\"name\"] for o in scen.objectives]\n",
    "\n",
    "df = scenario['run_result']\n",
    "performances = df.groupby([\"action\",\"configurators\", \"seeds\", \"configuration\"])[objectives].mean().unstack(\"action\").swaplevel(0, 1, 1)\n",
    "performances.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "performances = df.groupby([\"action\", \"seeds\", \"configurators\", \"configuration\"])[objectives].mean().reset_index()\n",
    "stats = performances.groupby(\"action\")[objectives].describe().stack()\n",
    "stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.set(font_scale=1.)\n",
    "\n",
    "hvs = []\n",
    "igdp_dict = {}\n",
    "partitions = [\"validate\", \"test\"]\n",
    "for scenario, partition in itertools.product(scenarios, partitions):\n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    # if scenario_name not in [\"MIP_CPLEX_REGIONS200_cutoff_50\", \"MIP_CPLEX_REGIONS200_runtime_50\", \"SAT_CMS_QUEENS_runtime_memory_50\"]:\n",
    "    #     continue\n",
    "    print(scenario_name)\n",
    "    print(scenariodef)\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "    print(objectives)\n",
    "\n",
    "    df = scenario[\"run_result\"]\n",
    "    print(len(df[df[\"configurators\"] == \"Default\"]))\n",
    "    df = df[~((df[\"configurators\"] == \"Default\") & (df[\"seeds\"] != 0))]\n",
    "    print(len(df[df[\"configurators\"] == \"Default\"]))\n",
    "    if \"ML_RF_STUDENTS_accuracy\" in scenario_name:\n",
    "        # Remove F1 from this scenario\n",
    "        df = df[~((df[\"configurators\"] == \"SMAC\") & (df[\"configuration\"] == 2))]\n",
    "    performances = df.groupby([\"action\",\"configurators\", \"seeds\", \"configuration\"])[objectives].mean().unstack(\"action\").swaplevel(0, 1, 1)\n",
    "\n",
    "    minimum_val = performances[\"validate\"].min()\n",
    "    maximum_val = performances[\"validate\"].max()\n",
    "\n",
    "    minimum = performances[partition].min()\n",
    "    maximum = performances[partition].max()\n",
    "\n",
    "    print(f\"{maximum=}\")\n",
    "    print(f\"{minimum=}\")\n",
    "    \n",
    "    #Export for trajectories\n",
    "    stats = df.groupby([\"action\", \"seeds\", \"configurators\", \"configuration\"])[objectives].mean().reset_index()\n",
    "    stats = stats.groupby(\"action\")[objectives].describe().stack()\n",
    "    stats_dest = Path(\"intermediates/norm_stats/\")\n",
    "    stats_dest.mkdir(exist_ok=True, parents=True)\n",
    "    stats.to_csv(stats_dest / f\"{scenario_name}.csv\")\n",
    "\n",
    "    points = performances[partition].to_numpy()\n",
    "    #Normalize objectives\n",
    "    for i, o in enumerate(objectives):\n",
    "        points[:, i] = 1 + ((points[:, i] - minimum[o]) / (maximum[o] - minimum[o]))\n",
    "    reference_set_ids = pygmo.fast_non_dominated_sorting(points)[0][0]\n",
    "    reference_set = points[reference_set_ids, :]\n",
    "\n",
    "    print(f\"{reference_set.shape=}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    raw_reference_set = copy.copy(reference_set)\n",
    "    for i, o in enumerate(objectives):\n",
    "        raw_reference_set[:, i] = minimum[o]+((raw_reference_set[:,i]-1)*(maximum[o]-minimum[o]))\n",
    "    ax.scatter(*raw_reference_set.T)\n",
    "    ax.set_title(f\"{scenario_replacements[str(scen)]}\")\n",
    "    ax.set_xlabel(objectives[0])\n",
    "    ax.set_ylabel(objectives[1])\n",
    "    if \"ML\" in str(scen):\n",
    "        if objectives[o1] in [\"accuracy\", \"recall\", \"precision\"]:\n",
    "            ax.set_xlabel(f\"1 - {objectives[0]}\")\n",
    "        if objectives[o2] in [\"accuracy\", \"recall\", \"precision\"]:\n",
    "            ax.set_ylabel(f\"1 - {objectives[1]}\")\n",
    "    if \"MIP\" in str(scen) and \"runtime\" in str(scen):\n",
    "        ax.set_ylim((0,10))\n",
    "    if objectives[1] == \"size\" and \"ML_RF\" in str(scen):\n",
    "        ax.set_yscale(\"log\")\n",
    "    fig.savefig(figurepath / f\"{partition}_fronts/refset_{scen}.pdf\", dpi=150, bbox_inches='tight', pad_inches=0)\n",
    "    fig.show()\n",
    "    # fig.clf()\n",
    "\n",
    "    igdplus = IGDPlus(reference_set)\n",
    "    igdp_dict[(scenario_name, partition)] = igdplus\n",
    "\n",
    "    figures = []\n",
    "    for _ in itertools.combinations(range(len(objectives)), r=2):\n",
    "        figures.append(plt.subplots(figsize=figsize))\n",
    "\n",
    "    # Sort configurators\n",
    "    performances[\"order\"] = performances.apply(lambda r: list(config_names.values()).index(r.name[0]) , axis=1)\n",
    "    performances = performances.sort_values(\"order\")\n",
    "\n",
    "    for (_, configurator), pdf in performances.groupby([\"order\", \"configurators\"]):\n",
    "        print(configurator)\n",
    "        #Compute ND set from validation performance\n",
    "        points = pdf[\"validate\"].to_numpy()\n",
    "        if len(points) > 1:\n",
    "            ndf, dl, dc, ndr = pygmo.fast_non_dominated_sorting(points)\n",
    "            front = ndf[0]\n",
    "        else:\n",
    "            front = [0]\n",
    "\n",
    "        #Get test performance\n",
    "        points = pdf[partition].to_numpy()\n",
    "        if np.any(np.isnan(points)):\n",
    "            print(f\"MISSING POINTS, SKIPPING {configurator}\")\n",
    "            continue\n",
    "\n",
    "        # Plot performance\n",
    "        for figid, objids in enumerate(itertools.combinations(range(len(objectives)), r=2)):\n",
    "            ndpoints = points[:, objids]\n",
    "            ndpoints = ndpoints[front, :]\n",
    "            ndpoints = ndpoints[np.argsort(ndpoints[:, 0])]\n",
    "\n",
    "            highlightpoints = ndpoints\n",
    "            dominatedpoints = []\n",
    "            if len(ndpoints) > 1:\n",
    "                ndpi = np.zeros(len(ndpoints), dtype=bool)\n",
    "                for i in pygmo.fast_non_dominated_sorting(ndpoints)[0][0]:\n",
    "                    ndpi[i] = True\n",
    "                highlightpoints = ndpoints[ndpi, :]\n",
    "                dominatedpoints = ndpoints[~ndpi, :]\n",
    "\n",
    "            highlightpoints = highlightpoints[np.argsort(highlightpoints[:, 0])]\n",
    "            steps = [highlightpoints[0,:]]\n",
    "            for i in range(1, len(highlightpoints)):\n",
    "                steps.append((highlightpoints[i,0], highlightpoints[i-1,1]))\n",
    "                steps.append(highlightpoints[i])\n",
    "            figures[figid][1].scatter(\n",
    "                *highlightpoints.T, \n",
    "                label=configurator, \n",
    "                marker=get_style(configurator, \"marker\"), \n",
    "                color=get_style(configurator, \"color\")\n",
    "            )\n",
    "            figures[figid][1].plot(*zip(*steps), color=get_style(configurator, \"color\"), alpha=0.25, linewidth=2)\n",
    "            if len(dominatedpoints) > 0:\n",
    "                figures[figid][1].scatter(*dominatedpoints.T, marker=get_style(configurator, \"marker\"), color=get_style(configurator, \"color\"), alpha=0.25)\n",
    "\n",
    "\n",
    "        #Compute HV and IGD+\n",
    "        #Normalize\n",
    "        ndpoints = points[front, :]\n",
    "        for i, o in enumerate(objectives):\n",
    "            ndpoints[:, i] = 1 + ((ndpoints[:, i] - minimum[o]) / (maximum[o] - minimum[o]))\n",
    "\n",
    "        hv = pygmo.hypervolume(ndpoints).compute([2.0+1e-1]*len(objectives))\n",
    "        igdp_score = igdplus(ndpoints)\n",
    "        hvs.append({\"scenario\": scenario_name,\n",
    "                    \"configurator\":configurator,\n",
    "                    \"partition\": partition,\n",
    "                    \"hypervolume\": hv,\n",
    "                    \"igdp\": igdp_score})\n",
    "\n",
    "\n",
    "\n",
    "    for figid, (o1, o2) in enumerate(itertools.combinations(range(len(objectives)), r=2)):\n",
    "        fig, ax = figures[figid]\n",
    "        # ax.set_title(scen)\n",
    "        ax.set_xlabel(objectives[o1])\n",
    "        ax.set_ylabel(objectives[o2])\n",
    "        if \"ML\" in str(scen):\n",
    "            if objectives[o1] in [\"accuracy\", \"recall\", \"precision\"]:\n",
    "                ax.set_xlabel(f\"1 - {objectives[o1]}\")\n",
    "            if objectives[o2] in [\"accuracy\", \"recall\", \"precision\"]:\n",
    "                ax.set_ylabel(f\"1 - {objectives[o2]}\")\n",
    "        if \"MIP\" in str(scen) and \"runtime\" in str(scen):\n",
    "            ax.set_ylim((0,10))\n",
    "        if objectives[o2] == \"size\" and \"ML_RF\" in str(scen):\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "        # ax.set_aspect(\"equal\")\n",
    "        # ax.set_aspect('equal', adjustable='datalim')\n",
    "        sns.set_style(\"whitegrid\")\n",
    "\n",
    "        # ax.legend()\n",
    "        # ax.set_title(f\"{str(scen)[:3].replace('_','')}-({objectives[o1]},{objectives[o2]})\")\n",
    "        ax.set_title(f\"{scenario_replacements[str(scen)]}\")\n",
    "        # ax.set_title(f\"{str(scen)[:3].replace('_','').replace(' (c)','')}-({', '.join(objectives)})\")\n",
    "        fig.tight_layout()\n",
    "        Path(figurepath / f\"{partition}_fronts/\").mkdir(exist_ok=True, parents=True)\n",
    "        fig.savefig(figurepath / f\"{partition}_fronts/{scen}_{o1}_{o2}.pdf\", dpi=150, bbox_inches='tight', pad_inches=0)\n",
    "        fig.show()\n",
    "        fig.clf()\n",
    "plt.close()\n",
    "qdf = pd.DataFrame(hvs)\n",
    "\n",
    "for k, v in scenario_replacements.items():\n",
    "    qdf.loc[qdf[\"scenario\"] == k, \"scenario\"] = v\n",
    "\n",
    "qdf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "qdf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate Tables\n",
    "def formatt(s):\n",
    "    return s.replace(\"_\", \"\\_\")\n",
    "\n",
    "fqdf = qdf[~qdf[\"configurator\"].isin([\"MO-SMAC\", \"Random Search\"])]\n",
    "configorder = [\"MO-SMAC-PHVI\", \"MO-SMAC-PE\", \"MO-ParamILS\", \"Random Search (25)\", \"SMAC\", \"Default\"]\n",
    "for (performance_measure, maximise, notation), (p, grp) in itertools.product([(\"hypervolume\", True, \".3f\"), (\"igdp\", False, \".2e\")], fqdf.groupby(\"partition\")):\n",
    "    print(p, performance_measure)\n",
    "    grp = grp.set_index([\"scenario\", \"configurator\"])[performance_measure].unstack(\"configurator\")\n",
    "    grp = grp[configorder]\n",
    "    display(grp)\n",
    "\n",
    "    table = \"\\\\begin{tabular}{l\" + \"r\"*grp.shape[1] + \"} \\\\toprule \\n\"\n",
    "    table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(h)}}}\" for h in grp.columns]) + \"\\\\\\\\\\n\"\n",
    "    table += \"\\\\midrule \\n\"\n",
    "    for scenario, row in grp.iterrows():\n",
    "        best_val = row.max() if maximise else row.min()\n",
    "        ranks = -row if maximise else row\n",
    "        ranks = np.argsort(np.argsort(ranks))\n",
    "        ranks = 1 + ranks\n",
    "        table += f\"\\\\bf{{{formatt(str(scenario))}}}\" + \" & \" + \" & \".join([f\"{v:.2e} ({r})\" if v != best_val or np.isnan(v) else f\"\\\\bf{{{v:.2e}}} ({r})\" for v, r in zip(row, ranks)]) + \"\\\\\\\\\\n\"\n",
    "    table += \"\\\\bottomrule \\n\\\\end{tabular}\\n\"\n",
    "\n",
    "    table_fn = Path(f\"tables/{performance_measure}_{p}.tex\")\n",
    "    table_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(table_fn, \"w\") as fh:\n",
    "        fh.write(table)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance stability of individual runs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "hvs = []\n",
    "partitions = [\"validate\", \"test\"]\n",
    "for scenario, partition in itertools.product(scenarios, partitions):\n",
    "    print()\n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    # if scenario_name not in [\"MIP_CPLEX_REGIONS200_cutoff_50\", \"MIP_CPLEX_REGIONS200_runtime_50\", \"SAT_CMS_QUEENS_runtime_memory_50\"]:\n",
    "    #     continue\n",
    "    print(scenario_name)\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "    print(objectives)\n",
    "\n",
    "    igdplus = igdp_dict[(scenario_name, partition)]\n",
    "\n",
    "    df = scenario[\"run_result\"]\n",
    "    #df = df[~((df[\"configurators\"] == \"default\") & (df[\"seeds\"] != \"0\"))]\n",
    "    if \"ML_RF_STUDENTS_accuracy\" in scenario_name:\n",
    "        # Remove F1 from this scenario\n",
    "        df = df[~((df[\"configurators\"] == \"SMAC\") & (df[\"configuration\"] == 2))]\n",
    "    performances = df.groupby([\"action\",\"configurators\", \"seeds\", \"configuration\"])[objectives].mean().unstack(\"action\").swaplevel(0, 1, 1)\n",
    "    minimum = performances.stack(\"action\").min()\n",
    "    maximum = performances.stack(\"action\").max()\n",
    "\n",
    "    print(\"max:\", maximum)\n",
    "    print(\"min:\", minimum)\n",
    "\n",
    "    print(df.columns)\n",
    "    print(\"configurators\", df[\"configurators\"].unique())\n",
    "\n",
    "    for (configurator, seed), pdf in performances.groupby([\"configurators\", \"seeds\"]):\n",
    "        if configurator == \"MO-SMAC\":\n",
    "            continue\n",
    "        #Get test performance\n",
    "        points = pdf[partition].to_numpy()\n",
    "        #Compute HV\n",
    "        #Normalize\n",
    "        ndpoints = points\n",
    "        for i, o in enumerate(objectives):\n",
    "            ndpoints[:, i] = 1 + ((ndpoints[:, i] - minimum[o]) / (maximum[o] - minimum[o]))\n",
    "\n",
    "        hv = pygmo.hypervolume(ndpoints).compute([2.0+1e-10]*len(objectives))\n",
    "        igdp_score = igdplus(ndpoints)\n",
    "        hvs.append({\"scenario\": scenario_name,\n",
    "                    \"configurator\":configurator,\n",
    "                    \"seed\": seed,\n",
    "                    \"partition\": partition,\n",
    "                    \"hypervolume\": hv,\n",
    "                    \"igdp\": igdp_score})\n",
    "\n",
    "\n",
    "\n",
    "qdf = pd.DataFrame(hvs)\n",
    "for k, v in scenario_replacements.items():\n",
    "    qdf.loc[qdf[\"scenario\"] == k, \"scenario\"] = v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "extremes = qdf.groupby([\"partition\",\"scenario\"])[[\"hypervolume\", \"igdp\"]].agg([\"min\", \"max\"])\n",
    "# qdf.apply(lambda r: extremes[(r[\"partition\"], r[\"scenario\"])], axis=1)\n",
    "def normalize(row):\n",
    "    se = extremes.loc[(row[\"partition\"], row[\"scenario\"])]\n",
    "    for p in [\"hypervolume\", \"igdp\"]:\n",
    "        row[p] =  (row[p] - se[(p, \"min\")]) / (se[(p, \"max\")] - se[(p, \"min\")])\n",
    "    return row\n",
    "\n",
    "nqdf = qdf.apply(normalize, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "qdf[(qdf[\"partition\"] == \"test\") & (qdf[\"scenario\"] == \"SAT_CMS_QUEENS_runtime_memory\") & (qdf[\"configurator\"] == \"SMAC\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Statistical test (not used in paper)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import scipy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "candidates = list(qdf[\"configurator\"].unique())\n",
    "\n",
    "for quality in [\"hypervolume\", \"igdp\"]:\n",
    "    partition = \"test\"\n",
    "    scenario = \"MIP_CPLEX_REGIONS200_cutoff\"\n",
    "    table = \"\\\\begin{tabular}{l\" + \"r\"*len(candidates) + \"} \\\\toprule \\n\"\n",
    "    for scenario in qdf[\"scenario\"].unique():\n",
    "        print(scenario)\n",
    "        perf = qdf[(qdf[\"partition\"] == partition) & (qdf[\"scenario\"] == scenario)].set_index([\"configurator\", \"seed\"])[[\"hypervolume\",\"igdp\"]].unstack(\"configurator\")\n",
    "\n",
    "        p_values = []\n",
    "        for c1, c2 in itertools.product(list(qdf[\"configurator\"].unique()), repeat=2):\n",
    "            test = scipy.stats.mannwhitneyu(perf[(quality, c1)], perf[(quality, c2)], alternative=\"two-sided\", method=\"exact\")\n",
    "            p_values.append({\"config1\": c1, \"config2\": c2, \"p_value\": test.pvalue})\n",
    "\n",
    "        p_values = pd.DataFrame(p_values).set_index([\"config1\", \"config2\"]).unstack(\"config2\")\n",
    "        display(p_values)\n",
    "\n",
    "        # table += \"\\\\begin{tabular}{l\" + \"r\"*len(candidates) + \"} \\\\toprule \\n\"\n",
    "        table += f\"& \\\\multicolumn{{{len(candidates)}}}{{c}}{{{formatt(scenario)}}} \\\\\\\\ \\\\midrule \\n\"\n",
    "        table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(h[1])}}}\" for h in p_values.columns]) + \"\\\\\\\\\\n\"\n",
    "        table += \"\\\\midrule \\n\"\n",
    "        for config1, row in p_values.iterrows():\n",
    "            significant = [\"S\" if p < 0.05 else \".\" for p in row]\n",
    "            table += f\"\\\\bf{{{formatt(str(config1))}}}\" + \" & \" + \" & \".join([f\"{v:.2e} ({r})\" if not np.isnan(v) else \"\" for v, r in zip(row, significant)]) + \"\\\\\\\\ \\n\"\n",
    "        table += \"\\\\midrule \\n\"\n",
    "\n",
    "    table += \"\\\\bottomrule \\n\\\\end{tabular}\\n\"\n",
    "\n",
    "    # print(table)\n",
    "\n",
    "    table_fn = Path(f\"tables/mannwhitneyu_{quality}.tex\")\n",
    "    table_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(table_fn, \"w\") as fh:\n",
    "        fh.write(table)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "candidates = list(qdf[\"configurator\"].unique())\n",
    "\n",
    "partition = \"test\"\n",
    "scenario = \"MIP_CPLEX_REGIONS200_cutoff\"\n",
    "\n",
    "for quality in [\"hypervolume\", \"igdp\"]:\n",
    "    table = \"\\\\begin{tabular}{l\" + \"r\"*len(candidates) + \"} \\\\toprule \\n\"\n",
    "    perf = qdf[(qdf[\"partition\"] == partition)].set_index([\"configurator\", \"scenario\", \"seed\"])[[\"hypervolume\",\"igdp\"]].unstack(\"configurator\")\n",
    "    p_values = []\n",
    "    for c1, c2 in itertools.product(list(qdf[\"configurator\"].unique()), repeat=2):\n",
    "        test = scipy.stats.mannwhitneyu(perf[(quality, c1)], perf[(quality, c2)], alternative=\"two-sided\", method=\"exact\")\n",
    "        p_values.append({\"config1\": c1, \"config2\": c2, \"p_value\": test.pvalue})\n",
    "\n",
    "    p_values = pd.DataFrame(p_values).set_index([\"config1\", \"config2\"]).unstack(\"config2\")\n",
    "    p_values\n",
    "\n",
    "    # table += \"\\\\begin{tabular}{l\" + \"r\"*len(candidates) + \"} \\\\toprule \\n\"\n",
    "    table += f\"& \\\\multicolumn{{{len(candidates)}}}{{c}}{{{formatt(quality)}}} \\\\\\\\ \\\\midrule \\n\"\n",
    "    table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(h[1])}}}\" for h in p_values.columns]) + \"\\\\\\\\\\n\"\n",
    "    table += \"\\\\midrule \\n\"\n",
    "    for config1, row in p_values.iterrows():\n",
    "        significant = [\"S\" if p < 0.05 else \".\" for p in row]\n",
    "        table += f\"\\\\bf{{{formatt(str(config1))}}}\" + \" & \" + \" & \".join([f\"{v:.2e} ({r})\" if not np.isnan(v) else \"\" for v, r in zip(row, significant)]) + \"\\\\\\\\ \\n\"\n",
    "\n",
    "    table += \"\\\\bottomrule \\n\\\\end{tabular}\\n\"\n",
    "\n",
    "    print(table)\n",
    "\n",
    "    table_fn = Path(f\"tables/mannwhitneyu_{quality}_combined.tex\")\n",
    "    table_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(table_fn, \"w\") as fh:\n",
    "        fh.write(table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "palette\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "figsize = (4, 2.5)\n",
    "xtickrotation = 0  # 35\n",
    "# palette = {conf: col for conf, col in zip(configurators, color_palette)}\n",
    "order = [conf for conf in list(config_names.values()) if conf in list(qdf[\"configurator\"].unique())]\n",
    "# for performance_measure, partition, (scenario, grp) in itertools.product([\"hypervolume\", \"igdp\"], [\"validate\", \"test\"], qdf.groupby(\"scenario\")):\n",
    "#     fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "#     sns.boxplot(\n",
    "#         data=grp[grp[\"partition\"] == partition],\n",
    "#         x=performance_measure,\n",
    "#         y=\"configurator\",\n",
    "#         palette=palette,\n",
    "#         order=order,\n",
    "#     )\n",
    "#     plt.xticks(rotation=xtickrotation)\n",
    "#     #place legend outside top right corner of plot\n",
    "#     # plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "#     # plt.title(f\"{performance_measure:} variance on {partition} set {scenario}\")\n",
    "#     plt.tight_layout()\n",
    "#     fn = Path(figurepath / f\"boxplots/{performance_measure}_{partition}_{scenario}.pdf\")\n",
    "#     fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "#     plt.savefig(fn)\n",
    "#     plt.close()\n",
    "dpi = 300\n",
    "\n",
    "def make_boxplot_figure(nqdf):\n",
    "    figsize = (7, 2.5)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    axes = fig.subplots(nrows=1, ncols=2, sharey=True, sharex=False)\n",
    "    for i, (ax, performance_measure) in enumerate(zip(axes, [\"hypervolume\", \"igdp\"])):\n",
    "        ax = sns.boxplot(\n",
    "            data=nqdf[nqdf[\"partition\"] == partition],\n",
    "            x=performance_measure,\n",
    "            y=\"configurator\",\n",
    "            palette=palette,\n",
    "            order=order,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        #place legend outside top right corner of plot\n",
    "        # plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "        # plt.title(f\"{performance_measure:} variance on all {partition} sets\")\n",
    "        # if performance_measure == \"igdp\":\n",
    "        #     ax.set_xlim((0, 0.5))\n",
    "        if i != 0:\n",
    "            ax.set_ylabel(None)\n",
    "        else:\n",
    "            pass\n",
    "            # labels = [config_names.get(item.get_text(), item.get_text()) for item in ax.get_yticklabels()]\n",
    "            # print(labels)\n",
    "            # ax.set_yticklabels(labels)\n",
    "        ax.set_xlabel(performance_measure)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "for partition in [\"validate\", \"test\"]:\n",
    "    fig = make_boxplot_figure(nqdf)\n",
    "    fn = Path(figurepath / f\"boxplots/combined_{partition}.pdf\")\n",
    "    fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    fig.savefig(fn, bbox_inches=\"tight\", dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "for partition, (scenario, grp) in itertools.product([\"validate\", \"test\"], qdf.groupby(\"scenario\")):\n",
    "    fig = make_boxplot_figure(grp)\n",
    "    fig.suptitle(scenario)\n",
    "    fig.tight_layout()\n",
    "    fn = Path(figurepath / f\"boxplots/combined_{partition}_{scenario}.pdf\")\n",
    "    fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    fig.savefig(fn, bbox_inches=\"tight\", dpi=dpi)\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nqdf[\"configurator\"].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML F1-score ranking"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import robustranking as rr",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for scenario, partition in itertools.product(scenarios, partitions):\n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    if scenario_name not in [\"ML_RF_STUDENTS_precision_recall\", \"ML_RF_STUDENTS_precision_recall_size\"]:\n",
    "        continue\n",
    "\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "\n",
    "    # igdplus = igdp_dict[(scenario_name, partition)]\n",
    "\n",
    "    df = scenario[\"run_result\"]\n",
    "    df = df[df[\"action\"] == partition]\n",
    "\n",
    "    df[\"f1\"] = 2 * ((1 - df[\"precision\"]) * (1 - df[\"recall\"])) / ((1 - df[\"precision\"]) + (1 - df[\"recall\"]))\n",
    "\n",
    "\n",
    "    print(scenario_name, partition)\n",
    "    mdf = df.groupby([\"configurators\", \"seeds\"])[\"f1\"].max().reset_index()\n",
    "    print(mdf.groupby([\"configurators\"]).apply(lambda x: x[\"f1\"].to_list()).to_dict())\n",
    "    # display(df.groupby([\"configurators\"])[\"f1\"].describe())#.sort_values(ascending=False))\n",
    "    benchmark = rr.Benchmark().from_pandas(df.groupby([\"configurators\", \"seeds\"])[\"f1\"].max().reset_index(), \"configurators\", \"seeds\", \"f1\")\n",
    "    comp = rr.comparison.RankedComparison(benchmark, minimise=False)\n",
    "    rr.utils.plots.plot_critical_difference(comp, alpha=0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Specify the algorithm to compare against\n",
    "baseline_algorithm = 'SMAC'\n",
    "alpha = 0.05\n",
    "for scenario, partition in itertools.product(scenarios, partitions):\n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    if scenario_name not in [\"ML_RF_STUDENTS_precision_recall\", \"ML_RF_STUDENTS_precision_recall_size\"]:\n",
    "        continue\n",
    "\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "\n",
    "    # igdplus = igdp_dict[(scenario_name, partition)]\n",
    "\n",
    "    df = scenario[\"run_result\"]\n",
    "    df = df[df[\"action\"] == partition]\n",
    "\n",
    "    df[\"f1\"] = 2 * ((1 - df[\"precision\"]) * (1 - df[\"recall\"])) / ((1 - df[\"precision\"]) + (1 - df[\"recall\"]))\n",
    "\n",
    "    print(scenario_name, partition)\n",
    "    mdf = df.groupby([\"configurators\", \"seeds\"])[\"f1\"].max().reset_index()\n",
    "    observations = mdf.groupby([\"configurators\"]).apply(lambda x: x[\"f1\"].to_list()).to_dict()\n",
    "\n",
    "    # Perform statistical tests\n",
    "    p_values_ttest = []\n",
    "    p_values_wilcoxon = []\n",
    "    algorithms = []\n",
    "    \n",
    "    for algorithm, data in observations.items():\n",
    "        if algorithm != baseline_algorithm:\n",
    "            # Paired t-test (one-sided)\n",
    "            t_stat, p_value_ttest = ttest_rel(observations[baseline_algorithm], data, alternative='less')\n",
    "            p_values_ttest.append(p_value_ttest)\n",
    "            \n",
    "            # Wilcoxon signed-rank test (one-sided)\n",
    "            try:\n",
    "                w_stat, p_value_wilcoxon = wilcoxon(observations[baseline_algorithm], data, alternative='less')\n",
    "            except ValueError:\n",
    "                w_stat, p_value_wilcoxon = np.nan, np.nan  # Handle edge cases (e.g., all zeros)\n",
    "            p_values_wilcoxon.append(p_value_wilcoxon)\n",
    "            \n",
    "            # Store algorithms for reference\n",
    "            algorithms.append(algorithm)\n",
    "    \n",
    "    # Apply multiple testing correction (Benjamini-Hochberg FDR)\n",
    "    corrected_ttest = multipletests(p_values_ttest, method='holm')\n",
    "    corrected_wilcoxon = multipletests(p_values_wilcoxon, method='holm',)\n",
    "    \n",
    "    # Store corrected results\n",
    "    alpha = 0.05\n",
    "    for i, algorithm in enumerate(algorithms):\n",
    "        # results[algorithm] = {\n",
    "        #     'ttestp': p_values_ttest[i],\n",
    "        #     'ttestpc': corrected_ttest[1][i],\n",
    "        #     'wilcoxonp': p_values_wilcoxon[i],\n",
    "        #     'wilcoxonc': corrected_wilcoxon[1][i]\n",
    "        # }\n",
    "\n",
    "        print(f\"{algorithm} VS. {baseline_algorithm}\")\n",
    "        print(\"\\tt-test\")\n",
    "        print(f\" > p-value (corrected): {corrected_ttest[1][i]} -> {'SIGNIFICANT' if corrected_ttest[1][i] < alpha else ''}\")\n",
    "        print(\"\\tWilcoxon\")\n",
    "        print(f\" > p-value (corrected): {corrected_ttest[1][i]} -> {'SIGNIFICANT' if corrected_ttest[1][i] < alpha else ''}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"df.groupby([\"configurators\", \"seeds\"])[\"f1\"].max().reset_index().groupby(\"configurators\")[\"f1\"].describe()\n",
    "\n",
    "benchmark = rr.Benchmark().from_pandas(df.groupby([\"configurators\", \"seeds\"])[\"f1\"].max().reset_index(), \"configurators\", \"seeds\", \"f1\")\n",
    "comp = rr.comparison.RankedComparison(benchmark, minimise=False)\n",
    "rr.utils.plots.plot_critical_difference(comp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ML precision recall with f1 score\n",
    "\n",
    "# Find scenarios\n",
    "for s in scenarios:\n",
    "    scenario = s\n",
    "    if s[\"experiment\"][\"scenarios\"] == \"ML_SVM_MNIST_precision_recall\":\n",
    "        break\n",
    "\n",
    "df = scenario[\"run_result\"]\n",
    "df[df[\"configurators\"] == \"SMAC\"][\"configuration\"]\n",
    "\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance with different run sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "config_names"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_samples = 190\n",
    "list_n_runs = list(range(1,21))\n",
    "hvs = []\n",
    "igdp_dict = {}\n",
    "partitions = [\"validate\", \"test\"]\n",
    "for scenario, partition in itertools.product(scenarios, partitions):\n",
    "    scenario_name = str(scenario['experiment'][\"scenarios\"])\n",
    "    print(scenario_name, partition)\n",
    "    \n",
    "    resultdir.joinpath(\"sampledruns\").mkdir(exist_ok=True, parents=True)\n",
    "    resultfile = resultdir / f\"sampledruns/{scenario_name}_{partition}.csv\"\n",
    "    if resultfile.exists():\n",
    "        print(\"LOAD CSV\")\n",
    "        ret = pd.read_csv(resultfile)\n",
    "        hvs.append(ret)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # if scenario_name not in [\"MIP_CPLEX_REGIONS200_cutoff_50\", \"MIP_CPLEX_REGIONS200_runtime_50\", \"SAT_CMS_QUEENS_runtime_memory_50\"]:\n",
    "    #     continue\n",
    "    print(scenario_name)\n",
    "    scen = getattr(scenariodef, scenario_name)()\n",
    "    objectives = [o[\"name\"] for o in scen.objectives]\n",
    "    print(objectives)\n",
    "\n",
    "    df = scenario[\"run_result\"]\n",
    "    # print(len(df[df[\"configurators\"] == \"default\"]))\n",
    "    # df = df[~((df[\"configurators\"] == \"default\") & (df[\"seeds\"] != 0))]\n",
    "    # print(len(df[df[\"configurators\"] == \"default\"]))\n",
    "    if \"ML_RF_STUDENTS_accuracy\" in scenario_name:\n",
    "        # Remove F1 from this scenario\n",
    "        df = df[~((df[\"configurators\"] == \"SMAC\") & (df[\"configuration\"] == 2))]\n",
    "    performances = df.groupby([\"action\",\"configurators\", \"seeds\", \"configuration\"])[objectives].mean().unstack(\"action\").swaplevel(0, 1, 1)\n",
    "\n",
    "    minimum_val = performances[\"validate\"].min()\n",
    "    maximum_val = performances[\"validate\"].max()\n",
    "\n",
    "    minimum = performances[partition].min()\n",
    "    maximum = performances[partition].max()\n",
    "\n",
    "    print(f\"{maximum=}\")\n",
    "    print(f\"{minimum=}\")\n",
    "\n",
    "    points = performances[partition].to_numpy()\n",
    "    #Normalize objectives\n",
    "    for i, o in enumerate(objectives):\n",
    "        points[:, i] = 1 + ((points[:, i] - minimum[o]) / (maximum[o] - minimum[o]))\n",
    "    reference_set_ids = pygmo.fast_non_dominated_sorting(points)[0][0]\n",
    "    reference_set = points[reference_set_ids, :]\n",
    "\n",
    "    print(f\"{reference_set.shape=}\")\n",
    "\n",
    "    igdplus = IGDPlus(reference_set)\n",
    "    igdp_dict[(scenario_name, partition)] = igdplus\n",
    "\n",
    "    # Sort configurators\n",
    "    # SAMPLE DF HERE\n",
    "    performances[\"order\"] = performances.apply(lambda r: list(config_names.values()).index(r.name[0]) , axis=1)\n",
    "    performances = performances.sort_values(\"order\")\n",
    "\n",
    "    # for runsize in range(2,20):\n",
    "    def compute_performance(runsize: int) -> pd.DataFrame:\n",
    "        all_runs = list(performances.reset_index()[\"seeds\"].unique())\n",
    "        all_combinations = list(itertools.combinations(all_runs, r=runsize))\n",
    "        rng = np.random.RandomState(0)\n",
    "        sample = rng.choice(len(all_combinations), size=min(n_samples, len(all_combinations)), replace=False)\n",
    "        combinations = [all_combinations[i] for i in sample]\n",
    "\n",
    "        results = []\n",
    "        for ((_, configurator), pdf), sample_id in itertools.product(performances.groupby([\"order\", \"configurators\"]), range(len(combinations))):\n",
    "            pdf = pdf.reset_index()\n",
    "            # print(f\"{sample_id=}, {combinations[sample_id]=}, {len(pdf)}\")\n",
    "            pdf = pdf[pdf[\"seeds\"].isin(combinations[sample_id])]\n",
    "            pdf = pdf.set_index([\"seeds\", \"configuration\"])\n",
    "            # print(f\"{len(pdf)}\")\n",
    "\n",
    "            # print(configurator)\n",
    "            #Compute ND set from validation performance\n",
    "            points = pdf[\"validate\"].to_numpy()\n",
    "            if len(points) > 1:\n",
    "                ndf, dl, dc, ndr = pygmo.fast_non_dominated_sorting(points)\n",
    "                front = ndf[0]\n",
    "            else:\n",
    "                front = [0]\n",
    "\n",
    "            #Get test performance\n",
    "            points = pdf[partition].to_numpy()\n",
    "            if np.any(np.isnan(points)):\n",
    "                print(f\"MISSING POINTS, SKIPPING {configurator}\")\n",
    "                continue\n",
    "\n",
    "            #Compute HV and IGD+\n",
    "            #Normalize\n",
    "            ndpoints = points[front, :]\n",
    "            for i, o in enumerate(objectives):\n",
    "                ndpoints[:, i] = 1 + ((ndpoints[:, i] - minimum[o]) / (maximum[o] - minimum[o]))\n",
    "\n",
    "            hv = pygmo.hypervolume(ndpoints).compute([2.0+1e-1]*len(objectives))\n",
    "            igdp_score = igdplus(ndpoints)\n",
    "            res = {\"scenario\": scenario_name,\n",
    "                        \"configurator\":configurator,\n",
    "                        \"partition\": partition,\n",
    "                        \"hypervolume\": hv,\n",
    "                        \"igdp\": igdp_score,\n",
    "                        \"runsize\": runsize,\n",
    "                        \"sample\": sample_id}\n",
    "            results.append(res)\n",
    "        results = pd.DataFrame(results)\n",
    "        return results\n",
    "    \n",
    "    with Pool() as pool:\n",
    "        ret = pool.map(compute_performance, list_n_runs)\n",
    "    pd.concat(ret).to_csv(resultfile, index=False)\n",
    "    hvs.extend(ret)\n",
    "\n",
    "qdf = pd.concat(hvs)\n",
    "qdf.to_csv(resultdir / \"sampledruns.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "qdf = pd.concat([pd.read_csv(f) for f in resultdir.joinpath(\"sampledruns\").iterdir()])\n",
    "# qdf = pd.read_csv(resultdir / \"sampledruns.csv\")\n",
    "qdf = rename_metadata(qdf)\n",
    "qdf[\"configurator\"] = qdf[\"configurator\"].replace(\"Random Search 25\", \"Random Search (25)\")\n",
    "qdf[\"configurator\"].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "palette"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# markers = {config_names[k]: v for k, v in markers.items()}\n",
    "# markers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "qdf = pd.concat([pd.read_csv(f) for f in resultdir.joinpath(\"sampledruns\").iterdir()])\n",
    "qdf = rename_metadata(qdf)\n",
    "qdf[\"configurator\"] = qdf[\"configurator\"].replace(\"Random Search 25\", \"Random Search\").replace(\"Random Search (25)\", \"Random Search\")\n",
    "# qdf = qdf[~qdf[\"configurator\"].isin([\"Random Search\"])]\n",
    "\n",
    "sns.set(font_scale=1.)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# figsize = (3.5, 2.35)\n",
    "dpi = 300\n",
    "\n",
    "xticks = range(1, 23, 4)\n",
    "\n",
    "for (scenario, grp), indicator in itertools.product(qdf.groupby(\"scenario\"), [\"hypervolume\", \"igdp\"]):\n",
    "    print(scenario, indicator)\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax = sns.lineplot(\n",
    "        data=grp[(grp[\"partition\"] == \"test\")], \n",
    "        x=\"runsize\", \n",
    "        y=indicator, \n",
    "        hue=\"configurator\", \n",
    "        style=\"configurator\", \n",
    "        ax=ax, \n",
    "        palette=palette, \n",
    "        linewidth=1.5, \n",
    "        dashes=False,\n",
    "        markers=markers, \n",
    "        markersize=5,\n",
    "        markeredgewidth=0,\n",
    "    )\n",
    "    # ax.set_title(scenario)\n",
    "    ylabel = indicator\n",
    "    if indicator == \"igdp\":\n",
    "        ylabel = \"IGD+\"\n",
    "    elif indicator == \"hypervolume\":\n",
    "        ylabel = \"Hypervolume\"\n",
    "    \n",
    "    ax.set_title(f\"{scenario.replace(' (c)','')}\") #{ylabel.upper()}\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlabel(\"n independent runs\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(1, 20)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    # ax.set_xticks(xticks)\n",
    "    fig.tight_layout()\n",
    "    figfn = Path(figurepath / f\"vary_runs/{indicator}_{scenario}.pdf\")\n",
    "    figfn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    fig.savefig(figfn, dpi=dpi, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Latex Inclusion"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# filenames = Path(figurepath / f\"vary_runs\").glob(\"*.pdf\")\n",
    "# \n",
    "# for filename in filenames:\n",
    "# \n",
    "#     metric, scenario = filename.stem.split(\"_\")\n",
    "# \n",
    "#     if \"hypervolume\" in str(filename):\n",
    "# \n",
    "#         # {indicator}_{scenario}\n",
    "# \n",
    "#         filename2 = str(filename).replace(\"hypervolume\", \"igdp\")\n",
    "# \n",
    "#         replacements = dict(\n",
    "#             floatpos = \"h\",\n",
    "#             figwidth = \"0.4\",\n",
    "#             figlabel = f\"fig:vary_runs_{scenario}\",\n",
    "#             # figlabel2 = f\"fig:vary_runs_{Path(filename2).stem}\",\n",
    "#             filename1 = str(filename),\n",
    "#             filename2 = filename2, \n",
    "#             figcaption = f\"Hypervolume (left) and IGD+ (right) for Scenario {scenario}\",\n",
    "#         )\n",
    "# \n",
    "#         \n",
    "# \n",
    "#         template = r\"\"\"\n",
    "# \\begin{figure}[floatpos]\n",
    "#     \\centering\n",
    "#     \\includegraphics[width=\\textwidth]{figures/legend.pdf}\\\\\n",
    "#     \\vspace{-0.4cm}\n",
    "#     \\includegraphics[width=figwidth\\textwidth]{filename1}\\qquad\n",
    "#     \\includegraphics[width=figwidth\\textwidth]{filename2}  \n",
    "# \\caption{figcaption}\n",
    "# \\label{figlabel}       \n",
    "# \\end{figure}\n",
    "#         \"\"\"\n",
    "#         for k, v in replacements.items():\n",
    "#             if \"caption\" in k:\n",
    "#                 v = v.replace(\"_\", \"\\_\")\n",
    "#             template = template.replace(k, v)\n",
    "# \n",
    "#         print(template)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "#     \"\"\"            \\begin{subfigure}[t]{0.48\\textwidth}\n",
    "#                 \\includegraphics[width=figwidth\\textwidth]{filename1}\\\\\n",
    "#                 \\caption{figcaption1}\n",
    "#                 \\label{figlabel1}            \n",
    "#             \\end{subfigure}\\hfill\n",
    "#             \\begin{subfigure}[t]{0.48\\textwidth}\n",
    "#                 \\includegraphics[width=figwidth\\textwidth]{filename2}\\\\\n",
    "#                 \\caption{figcaption2}\n",
    "#                 \\label{figlabel2}            \n",
    "#             \\end{subfigure}     \n",
    "#     \"\"\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "qdf\n",
    "qdf[\"configurator\"].unique()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import scipy\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def formatt(s):\n",
    "    return s.replace(\"_\", \"\\_\")\n",
    "\n",
    "configorder = [\"MO-SMAC-PHVI\", \"MO-SMAC-PE\", \"MO-ParamILS\", \"Random Search\", \"SMAC\", \"Default\"]\n",
    "\n",
    "fqdf = qdf[(qdf[\"partition\"] == \"test\") & (qdf[\"runsize\"] == 1)]\n",
    "\n",
    "\n",
    "table = \"\\\\begin{tabular}{l\" + \"r\"*len(configorder) + \"} \\\\toprule \\n\"\n",
    "table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(c)}}}\" for c in configorder]) + \"\\\\\\\\\\n\"\n",
    "table += \"\\\\midrule \\n\"\n",
    "for indicator, maximise in [(\"hypervolume\", True), (\"igdp\", False)]:\n",
    "    table += f\"& \\\\multicolumn{{{len(configorder)}}}{{c}}{{{indicator}}} \\\\\\\\ \\\\midrule \\n\"\n",
    "    all_rankings = []\n",
    "    for scenario, grp in fqdf.groupby(\"scenario\"):\n",
    "        print(scenario, indicator)\n",
    "        grp = grp.set_index([\"configurator\", \"sample\"])[indicator].unstack(\"configurator\")[configorder]\n",
    "        mean = grp.mean()\n",
    "        # display(mean)\n",
    "        best = np.argmax(mean) if maximise else np.argmin(mean)\n",
    "        best_conf = mean.keys()[best]\n",
    "        # display(best_conf)\n",
    "\n",
    "        direction = -1 if maximise else 1\n",
    "        ranking = 1 + np.argsort(np.argsort(direction*mean))\n",
    "        all_rankings.append(ranking)\n",
    "        # display(ranking)\n",
    "\n",
    "        # statistical test\n",
    "        values = grp.to_numpy().T\n",
    "        # print(f\"{values.shape=}\")\n",
    "        a = values[best]\n",
    "        p_values = []\n",
    "        for i in range(len(values)):\n",
    "            b = values[i]\n",
    "\n",
    "            # print(f\"{a.shape=}, {b.shape=}, \")\n",
    "            test = scipy.stats.mannwhitneyu(a, b, alternative=\"two-sided\")\n",
    "            p_values.append(test.pvalue)\n",
    "        # display(p_values)\n",
    "        del p_values[best] # remove self comparison\n",
    "        # multiple test correction\n",
    "        significant, _, _, _ = multipletests(p_values, alpha=0.05, method='holm', is_sorted=False, returnsorted=False)\n",
    "        # display(significant)\n",
    "        significant = list(significant)\n",
    "        significant.insert(best, False)\n",
    "        display(significant)\n",
    "        #Now we can add the table line\n",
    "\n",
    "        elements = []\n",
    "        for i, (m, r, s) in enumerate(zip(mean, ranking, significant)):\n",
    "            string = f\"{m:.3f} ({r})\"\n",
    "            if i == best:\n",
    "                string = f\"\\\\textbf{{{string}}}\"\n",
    "            elif not s: # we cannot say it is significantly worse\n",
    "                string = f\"\\\\textbf{{{string}}}\"\n",
    "                # string += \" \\\\dag\"\n",
    "            elements.append(string)\n",
    "\n",
    "        table+= f\"\\\\textbf{{{scenario}}} &\" + \" & \".join(elements) + \" \\\\\\\\ \\n\"\n",
    "\n",
    "    #Average rankings\n",
    "    all_rankings = np.array(all_rankings)\n",
    "    print(all_rankings)\n",
    "    table += \"\\\\multicolumn{1}{r}{\\\\it Average ranking} &\" + \" & \".join([f\"{ar:.1f}\" for ar in np.mean(all_rankings, axis=0)]) + \"\\\\\\\\ \\\\midrule \\n\"\n",
    "\n",
    "table += \"\\\\bottomrule \\n\\\\end{tabular}\\n\"\n",
    "\n",
    "table_fn = Path(f\"tables/resulttable_8runs.tex\")\n",
    "table_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "with open(table_fn, \"w\") as fh:\n",
    "    fh.write(table)\n",
    "\n",
    "print(table)\n",
    "#\n",
    "# table = \"\\\\begin{tabular}{l\" + \"r\"*grp.shape[1] + \"} \\\\toprule \\n\"\n",
    "# table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(c)}}}\" for c in configorder]) + \"\\\\\\\\\\n\"\n",
    "# table += \"\\\\midrule \\n\"\n",
    "# for indicator in [\"hypervolume\", \"igdp\"]:\n",
    "#     continue\n",
    "#\n",
    "# print(table)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate Tables\n",
    "def formatt(s):\n",
    "    return s.replace(\"_\", \"\\_\")\n",
    "\n",
    "fqdf = qdf[qdf[\"configurator\"] != \"MO-SMAC\"]\n",
    "\n",
    "for (performance_measure, maximise, notation), (p, grp) in itertools.product([(\"hypervolume\", True, \".3f\"), (\"igdp\", False, \".2e\")], fqdf.groupby(\"partition\")):\n",
    "    print(p, performance_measure)\n",
    "    grp = grp.set_index([\"scenario\", \"configurator\"])[performance_measure].unstack(\"configurator\")\n",
    "    grp = grp[configorder]\n",
    "    display(grp)\n",
    "\n",
    "    table = \"\\\\begin{tabular}{l\" + \"r\"*grp.shape[1] + \"} \\\\toprule \\n\"\n",
    "    table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(config_names[h])}}}\" for h in grp.columns]) + \"\\\\\\\\\\n\"\n",
    "    table += \"\\\\midrule \\n\"\n",
    "    for scenario, row in grp.iterrows():\n",
    "        best_val = row.max() if maximise else row.min()\n",
    "        ranks = -row if maximise else row\n",
    "        ranks = np.argsort(np.argsort(ranks))\n",
    "        ranks = 1 + ranks\n",
    "        table += f\"\\\\bf{{{formatt(str(scenario))}}}\" + \" & \" + \" & \".join([f\"{v:.2e} ({r})\" if v != best_val or np.isnan(v) else f\"\\\\bf{{{v:.2e}}} ({r})\" for v, r in zip(row, ranks)]) + \"\\\\\\\\\\n\"\n",
    "    table += \"\\\\bottomrule \\n\\\\end{tabular}\\n\"\n",
    "\n",
    "    table_fn = Path(f\"tables/{performance_measure}_{p}.tex\")\n",
    "    table_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(table_fn, \"w\") as fh:\n",
    "        fh.write(table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurator run statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "resultdir = Path(\"results\")\n",
    "\n",
    "df = []\n",
    "for d in tqdm(resultdir.joinpath(\"configure\").iterdir()):\n",
    "    # print(f\"> {d}\")\n",
    "    cont = False\n",
    "    for conf in [\"MO-SMAC\", \"ParEGO\", \"SMAC\", \"MO-SMAC-PHVI\"]:\n",
    "    # for conf in [\"SAT-CMS-QUEENS-runtime-memory_SMAC_\"]:\n",
    "        if conf in str(d):\n",
    "            #print(f\"\\t{conf} in there\")\n",
    "            cont = True\n",
    "    if not cont:\n",
    "        continue\n",
    "\n",
    "    if \"test\" in str(d):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(d, \"rb\") as fh:\n",
    "            r = pickle.load(fh)\n",
    "            # print(r[\"experiment\"])\n",
    "        if r[\"experiment\"][\"configurators\"] == \"SMAC\":\n",
    "            # print(r[\"run_result\"])\n",
    "            # input()\n",
    "            continue\n",
    "        runhistory = r[\"run_result\"][\"runhistory\"]\n",
    "        runningtime = 0\n",
    "        diff = []\n",
    "        for trialkey, trial in runhistory.items():\n",
    "            runningtime += trial.endtime - trial.starttime\n",
    "            diff += [trial.starttime, trial.endtime]\n",
    "        #\n",
    "        # print(f\"ta runtime: {runningtime / 3600} hours\")\n",
    "        # print(f\"{len(runhistory)=}\")\n",
    "        # print(f\"Configurations: {runhistory._n_id}\")\n",
    "\n",
    "        diff = np.array(diff)\n",
    "        overhead = np.diff(diff)[1::2]\n",
    "\n",
    "        r[\"experiment\"][\"taruntime\"] = runningtime\n",
    "        r[\"experiment\"][\"ntrials\"] = len(runhistory)\n",
    "        r[\"experiment\"][\"nconfigs\"] = runhistory._n_id\n",
    "        r[\"experiment\"][\"overhead\"] = sum(np.sort(overhead)[::-1])\n",
    "        r[\"experiment\"][\"configgens\"] = np.count_nonzero(overhead > 0.25)\n",
    "\n",
    "        origins = [runhistory.get_config(i).origin for i in range(1, runhistory._n_id+1)]\n",
    "        for org, count in zip(*np.unique(origins, return_counts=True)):\n",
    "            r[\"experiment\"][org] = count / runhistory._n_id\n",
    "\n",
    "        df.append(r[\"experiment\"])\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "        # print(f\"{d} uses old version of SMAC\")\n",
    "\n",
    "df = pd.DataFrame(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Statistics\n",
    "df[\"refreshfreq\"] = df[\"nconfigs\"] / df[\"configgens\"]\n",
    "tdf = df.groupby([\"scenarios\", \"configurators\"]).agg([\"mean\"]).reset_index()\n",
    "# sns.boxplot(date=tdf, x=[\"\"])\n",
    "tdf\n",
    "# No.times Retraining of acquisition model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cols = [\"scenarios\", \"configurators\", \"taruntime\", \"ntrials\", \"nconfigs\", \"Acquisition Function Maximizer: Local Search\", \"Acquisition Function Maximizer: Random Search (sorted)\", \"Random Search\"]\n",
    "\n",
    "def formatt(s):\n",
    "    return s.replace(\"_\", \"\\_\")\n",
    "\n",
    "table = \"\\\\begin{tabular}{l\" + \"r\"*len(cols) + \"} \\\\toprule \\n\"\n",
    "table += \" & \" + \" & \".join([f\"\\\\bf{{{formatt(h)}}}\" for h in cols]) + \"\\\\\\\\\\n\"\n",
    "table += \"\\\\midrule \\n\"\n",
    "for _, grp in tdf[cols].iterrows():\n",
    "    values = []\n",
    "    for key, val in grp.to_dict().items():\n",
    "        if isinstance(val, str):\n",
    "            values.append(formatt(val))\n",
    "        else:\n",
    "            values.append(f\"{val:.2f}\")\n",
    "\n",
    "    table += \" & \".join(values) + \"\\\\\\\\\\n\"\n",
    "table += \"\\\\bottomrule \\n\\\\end{tabular}\\n\"\n",
    "\n",
    "with open(f\"tables/configurator_stats.tex\", \"w\") as fh:\n",
    "        fh.write(table)\n",
    "\n",
    "print(table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analyses"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "t = r[\"run_result\"][\"trajectory\"]\n",
    "trajectory = t[\"trajectory\"]\n",
    "trajectory_index = 0\n",
    "incumbent = trajectory[trajectory_index]\n",
    "\n",
    "unique_isb_pairs = set()\n",
    "incumbent_run = 0\n",
    "config_run = 0\n",
    "progress = []\n",
    "\n",
    "config_trial_counter = {}\n",
    "\n",
    "for trial_id, (tinfo, tresult) in enumerate(runhistory.items()):\n",
    "    challenger_added_to_incumbent = False\n",
    "\n",
    "    if trial_id == 0:\n",
    "        print(trial_id)\n",
    "        print(tinfo)\n",
    "        print(tresult)\n",
    "        continue\n",
    "\n",
    "    if tinfo.config_id in incumbent[\"config_ids\"]:\n",
    "        incumbent_run += 1\n",
    "    else:\n",
    "        config_run += 1\n",
    "        if tinfo.config_id not in config_trial_counter:\n",
    "            config_trial_counter[tinfo.config_id] = 0\n",
    "        config_trial_counter[tinfo.config_id] += 1\n",
    "\n",
    "    isb_key = f\"{tinfo.instance}_{tinfo.seed}\"\n",
    "    unique_isb_pairs.add(isb_key)\n",
    "\n",
    "    #Incumbent changes after running a trial\n",
    "    if trial_id >= incumbent[\"trial\"]:\n",
    "        trajectory_index = min(trajectory_index+1, len(trajectory)-1)\n",
    "        incumbent = trajectory[trajectory_index]\n",
    "        if tinfo.config_id in incumbent[\"config_ids\"]:\n",
    "            challenger_added_to_incumbent = True\n",
    "\n",
    "    stats = {\n",
    "        \"unique_isb_keys\": len(unique_isb_pairs),\n",
    "        \"incumbent_run\": incumbent_run,\n",
    "        \"config_run\": config_run,\n",
    "        \"incumbent_size\": len(incumbent[\"config_ids\"]),\n",
    "        \"challenger_added_to_incumbent\": challenger_added_to_incumbent\n",
    "    }\n",
    "\n",
    "    progress.append(stats)\n",
    "\n",
    "progress = pd.DataFrame(progress)\n",
    "\n",
    "progress[\"incumbent_size\"].plot()\n",
    "\n",
    "np.unique(list(config_trial_counter.values()), return_counts=True)\n",
    "\n",
    "#progress[progress[\"challenger_added_to_incumbent\"]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analyse_runhistory_isb_keys(r):\n",
    "    print(\"Experiment:\", r[\"experiment\"][\"scenarios\"], r[\"experiment\"][\"configurators\"],  r[\"experiment\"][\"seeds\"], )\n",
    "    print(r[\"run_result\"].keys())\n",
    "    t = r[\"run_result\"][\"trajectory\"]\n",
    "    trajectory = t[\"trajectory\"]\n",
    "    runhistory = r[\"run_result\"][\"runhistory\"]\n",
    "\n",
    "    config_trial_counter_seq = []\n",
    "    config_trial_counter_all = {}\n",
    "    current_config = np.nan\n",
    "    for trial_id, (tinfo, tresult) in enumerate(runhistory.items()):\n",
    "        if tinfo.config_id is not current_config:\n",
    "            config_trial_counter_seq.append(0)\n",
    "            current_config = tinfo.config_id\n",
    "\n",
    "        config_trial_counter_seq[-1] += 1\n",
    "\n",
    "        if tinfo.config_id not in config_trial_counter_all:\n",
    "            config_trial_counter_all[tinfo.config_id] = set()\n",
    "        config_trial_counter_all[tinfo.config_id].add(f\"{tinfo.instance}_{tinfo.seed}\")\n",
    "\n",
    "    print(\"Run status counts\")\n",
    "    rlist = [tresult.status for _, tresult in runhistory.items()]\n",
    "    print(list(zip(*np.unique(rlist, return_counts=True))))\n",
    "    print(\"Sequential trial count of a config\")\n",
    "    print(list(zip(*np.unique(config_trial_counter_seq, return_counts=True))))\n",
    "    print(\"All trials counts per config\")\n",
    "    hist_count = list(zip(*np.unique([len(c) for c in config_trial_counter_all.values()], return_counts=True)))\n",
    "    print(hist_count)\n",
    "    # for n_trials, count in hist_count:\n",
    "    #     print(f\"{n_trials:3}: {count}\")\n",
    "    print(\"Unique ISB keys\")\n",
    "    all_unique_keys = set()\n",
    "    for isb_keys in config_trial_counter_all.values():\n",
    "        all_unique_keys = all_unique_keys.union(isb_keys)\n",
    "    print(len(all_unique_keys))\n",
    "\n",
    "    maximum_runs = max([h[0] for h in hist_count])\n",
    "    intersection_keys = all_unique_keys\n",
    "    print(f\"Intersection between config with most runs ({maximum_runs})\")\n",
    "    for isb_keys in config_trial_counter_all.values():\n",
    "        if len(isb_keys) == maximum_runs:\n",
    "            intersection_keys = intersection_keys.intersection(isb_keys)\n",
    "    print(len(intersection_keys))\n",
    "\n",
    "    for inc_id, incumbent in enumerate(trajectory):\n",
    "        incumbent_trials = {i:set() for i in incumbent[\"config_ids\"]}\n",
    "\n",
    "        for trial_id, (tinfo, tresult) in enumerate(runhistory.items()):\n",
    "            if trial_id == incumbent[\"trial\"]:\n",
    "                break\n",
    "\n",
    "            if tinfo.config_id in incumbent[\"config_ids\"]:\n",
    "                incumbent_trials[tinfo.config_id].add(f\"{tinfo.instance}_{tinfo.seed}\")\n",
    "\n",
    "        union_isb = set()\n",
    "        intersect_isb = all_unique_keys\n",
    "\n",
    "        for isb_keys in incumbent_trials.values():\n",
    "            union_isb = union_isb.union(isb_keys)\n",
    "            intersect_isb = intersect_isb.intersection(isb_keys)\n",
    "\n",
    "        print(f\"Incumbent {inc_id} of size {len(incumbent_trials)} ran on {len(union_isb)} different isb_keys which share {len(intersect_isb)} isb_keys\")\n",
    "        for config_id, isb_keys in incumbent_trials.items():\n",
    "            overlap_with_others = [len(isb_keys.intersection(k)) for cid, k in incumbent_trials.items() if cid is not config_id]\n",
    "            if len(overlap_with_others) == 0:\n",
    "                overlap_with_others = [0]\n",
    "\n",
    "            print(f\"\\t Config {config_id:4} has {len(isb_keys):3} runs. Maximum overlap pair {max(overlap_with_others)}\")\n",
    "analyse_runhistory_isb_keys(r)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import contextlib\n",
    "scp = [(s,c) for s,c in itertools.product(df[\"scenarios\"].unique().tolist(), df[\"configurators\"].unique().tolist())]\n",
    "\n",
    "scp = set(scp)\n",
    "\n",
    "for d in tqdm(resultdir.joinpath(\"configure\").iterdir()):\n",
    "    cont = False\n",
    "    for c in [\"SMAC\", \"ParEGO\"]:\n",
    "        if c in str(d):\n",
    "            cont = True\n",
    "\n",
    "    if not cont:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(d, \"rb\") as fh:\n",
    "            r = pickle.load(fh)\n",
    "\n",
    "            sck = (r[\"experiment\"][\"scenarios\"], r[\"experiment\"][\"configurators\"])\n",
    "            if sck in scp:\n",
    "                scp.remove(sck)\n",
    "\n",
    "                with open(f\"trajectory_analysis/{sck[0]}_{sck[1]}_analysis.txt\",\"w\") as wfh:\n",
    "                    with contextlib.redirect_stdout(wfh):\n",
    "                        analyse_runhistory_isb_keys(r)\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
